{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST on Python 3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'mnist.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([5, 0, 4, ..., 8, 4, 8], dtype=int64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load USPS on Python 3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USPSMat  = []\n",
    "USPSTar  = []\n",
    "curPath  = 'USPSdata/Numerals'\n",
    "savedImg = []\n",
    "\n",
    "for j in range(0,10):\n",
    "    curFolderPath = curPath + '/' + str(j)\n",
    "    imgs =  os.listdir(curFolderPath)\n",
    "    for img in imgs:\n",
    "        curImg = curFolderPath + '/' + img\n",
    "        if curImg[-3:] == 'png':\n",
    "            img = Image.open(curImg,'r')\n",
    "            img = img.resize((28, 28))\n",
    "            savedImg = img\n",
    "            imgdata = (255-np.array(img.getdata()))/255\n",
    "            USPSMat.append(imgdata)\n",
    "            USPSTar.append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = training_data[0]\n",
    "train_tar = training_data[1]\n",
    "val_feat = validation_data[0]\n",
    "val_tar = validation_data[1]\n",
    "test_feat = test_data[0]\n",
    "test_tar = test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data[0][1,:]))#each image is of 28*28 --> so 784 features for each image(i.e., for each sample). 1 here represents the first image\n",
    "print(len(training_data[1])) #training_data[1] is the ACTUAL target value for each image and this is also 50,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing one-hot encoding on the target values (i.e., 10 classes)\n",
    "def onehot(tar):\n",
    "    n = tar.shape[0]\n",
    "    oh = scipy.sparse.csr_matrix((np.ones(n), (tar, np.array(range(n)))))\n",
    "    oh = np.array(oh.todense()).T\n",
    "    return oh\n",
    "\n",
    "def softmax(x):\n",
    "    x -= np.max(x)\n",
    "    softmax = (np.exp(x).T / np.sum(np.exp(x), axis = 1)).T\n",
    "    return softmax\n",
    "\n",
    "#finding probabilities and predictions given a set of input data\n",
    "def probsandpreds(features):\n",
    "    probabilities = softmax(np.dot(features, w)) #a vector of 10 probabilities corresponding to each class\n",
    "    predictions = np.argmax(probabilities, axis = 1) #maximum probability as the class\n",
    "    return probabilities, predictions\n",
    "\n",
    "def Loss(w, feat, tar, la):\n",
    "    n = feat.shape[0]\n",
    "    tar_oh = onehot(tar)\n",
    "    Y = np.dot(feat, w) #predicting target using linear regression \n",
    "    probs = softmax(Y)\n",
    "    loss = (-1/n)  * np.sum(tar_oh * np.log(probs)) + ((la/2) * np.sum(w*w))\n",
    "    gradient = (-1/n) * np.dot(feat.T, (tar_oh - probs)) + la*w\n",
    "    return loss, gradient\n",
    "\n",
    "#Accuracy\n",
    "def GetAccuracy(x, y):\n",
    "    probs, preds = probsandpreds(x)\n",
    "    accuracy = sum(preds == y)/(float(len(y)))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.zeros([train_feat.shape[1], len(np.unique(train_tar))]) #w will be a matrix from each feature to all the output classes, so 784x10\n",
    "la = 10\n",
    "learningRate = 0.01\n",
    "losses = []\n",
    "x1 = 0\n",
    "xn = 256\n",
    "\n",
    "for i in range(x1, xn):\n",
    "    loss, gradient = Loss(w, train_feat, train_tar, la)\n",
    "    losses.append(loss)\n",
    "    w = w - (learningRate * gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR(train_feat, train_tar):\n",
    "    for i in range(x1, xn):\n",
    "        loss, gradient = Loss(w, train_feat, train_tar, la)\n",
    "        losses.append(loss)\n",
    "        w = w - (learningRate * gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Logistic Regression using Stochastic Gradient Descent --------------------\n",
      "Lambda = 0.0390625\n",
      "eta = 0.01\n",
      "Validation Accuracy = 73.99\n",
      "Testing Accuracy = 0.7235\n",
      "USPS Accuracy = 0.24251212560628033\n"
     ]
    }
   ],
   "source": [
    "print ('---------- Logistic Regression using Stochastic Gradient Descent --------------------')\n",
    "print(\"Lambda = \" + str(la/np.subtract(xn, x1))) # lambda is La/no. of samples\n",
    "print(\"eta = \" + str(learningRate))\n",
    "print(\"Validation Accuracy = \" + str(GetAccuracy(val_feat, val_tar)*100))\n",
    "print(\"Testing Accuracy = \" + str(GetAccuracy(test_feat, test_tar)*100))\n",
    "print(\"USPS Accuracy = \" + str(GetAccuracy(USPSMat, USPSTar)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix of Validation Data: \n",
      "\n",
      "[[957   0   2   5   0   0   7   2  18   0]\n",
      " [  1 954   7  16   0   0   4   1  81   0]\n",
      " [ 75  10 773  38   3   0  31   9  45   6]\n",
      " [ 49   4  17 882   0   0   3   2  65   8]\n",
      " [ 59   8   3   4 578   0  41   1  78 211]\n",
      " [400   7  23 242   5   0  28   5 178  27]\n",
      " [ 96   9  12   3   0   0 830   0  17   0]\n",
      " [ 73  21  15  10   2   0   1 877  55  36]\n",
      " [ 50  13  10  97   0   0   7   4 822   6]\n",
      " [ 74   7   8  27   7   0   0  36  76 726]]\n",
      "\n",
      "Confusion Matrix of Testing Data: \n",
      "\n",
      "[[963   0   2   4   0   0   4   0   7   0]\n",
      " [  0 994   6  18   0   0   5   0 112   0]\n",
      " [ 96   5 749  70   4   0  34   6  65   3]\n",
      " [ 54   1  19 865   0   0   7   7  51   6]\n",
      " [ 54   6   9   7 534   0  58   2  93 219]\n",
      " [366   9  14 247   1   0  30   9 195  21]\n",
      " [134   4  11   4   2   0 781   0  22   0]\n",
      " [ 47  26  33   7   3   0   1 814  55  42]\n",
      " [ 65   2   6 106   1   0  16   8 761   9]\n",
      " [ 70   6  11  25  14   0   3  39  67 774]]\n",
      "\n",
      " Confusion Matrix of USPS Data: \n",
      "\n",
      "[[1392    2  259   54  107    0   24   12   58   92]\n",
      " [ 484  197  236  251   42    0   32  336  413    9]\n",
      " [ 811   18  825   95   10    0   77   47  112    4]\n",
      " [ 804    1   76  886    7    0   24   27  145   30]\n",
      " [ 620   56   69  123  514    0   30  141  382   65]\n",
      " [1139   12  166  348    6    0   74   29  208   18]\n",
      " [1333    5  210   57   36    0  293    5   56    5]\n",
      " [ 455  180  400  358    8    0   48  187  354   10]\n",
      " [ 934   19  190  210   28    0  108   16  477   18]\n",
      " [ 394  153  177  490   32    0   12  240  423   79]]\n"
     ]
    }
   ],
   "source": [
    "print('########################### CONFUSION MATRICES FOR LOGISTIC REGRESSION ###########################')\n",
    "probs_val, preds_val = probsandpreds(val_feat)\n",
    "print(\"\\nConfusion Matrix of Validation Data: \\n\\n\" + str(confusion_matrix(val_tar, preds_val)))\n",
    "probs_t, preds_t = probsandpreds(test_feat)\n",
    "print(\"\\nConfusion Matrix of Testing Data: \\n\\n\" + str(confusion_matrix(test_tar, preds_t)))\n",
    "probs_usps, preds_usps = probsandpreds(USPSMat)\n",
    "print(\"\\n Confusion Matrix of USPS Data: \\n\\n\" + str(confusion_matrix(USPSTar, preds_usps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPORT VECTOR MACHINE (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=123, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "C = 0.1\n",
    "gamma = 0.1\n",
    "clf = svm.SVC(kernel='linear', C=C, gamma = gamma, random_state = 123)\n",
    "clf.fit(train_feat, train_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Support Vector Machine (SVM) --------------------\n",
      "Regularization Parameter/Penalty(C) = 0.1\n",
      "gamma = 0.1\n",
      "SVM Validation Accuracy:  94.81\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Getting Validation dataset accuracy\n",
    "val_pred_svm = clf.predict(val_feat)\n",
    "acc_val_svm = accuracy_score(val_tar, val_pred_svm)\n",
    "\n",
    "# # Getting Testing dataset accuracy\n",
    "# test_pred_svm = clf.predict(test_feat)\n",
    "# acc_test_svm = accuracy_score(test_tar, test_pred_svm)\n",
    "\n",
    "# # Getting USPS dataset Accuracy\n",
    "# usps_pred_svm = clf.predict(USPSMat)\n",
    "# usps_acc_svm = accuracy_score(USPSTar, usps_pred_svm)\n",
    "\n",
    "print ('---------- Support Vector Machine (SVM) --------------------')\n",
    "print(\"Regularization Parameter/Penalty(C) = \" + str(C))\n",
    "print(\"gamma = \" + str(gamma))\n",
    "print(\"SVM Validation Accuracy: \", acc_val_svm*100)\n",
    "# print(\"SVM Test Accuracy: \", acc_test_svm*100)\n",
    "#print(\"SVM USPS Accuracy: \", usps_acc_svm*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix of Validation Data: \n",
      "\n",
      "[[ 968    0    6    0    2    4    8    0    1    2]\n",
      " [   0 1045    3    5    1    0    0    3    7    0]\n",
      " [   6   10  924   12    4    3    8    9   11    3]\n",
      " [   5    6   17  939    1   37    0    2   19    4]\n",
      " [   3    7   12    0  921    1    4    5    5   25]\n",
      " [  13    4    9   41    3  810   17    2   13    3]\n",
      " [   7    2   10    1   12   10  924    0    1    0]\n",
      " [   2    4   14   15    8    3    0 1025    1   18]\n",
      " [   6   16   14   29    2   26    4    6  897    9]\n",
      " [   4    6    5   11   31    5    1   37    8  853]]\n",
      "\n",
      "Confusion Matrix of Testing Data: \n",
      "\n",
      "[[ 955    0    6    1    0    7    8    1    1    1]\n",
      " [   0 1116    5    4    0    1    2    1    6    0]\n",
      " [   6   13  957   14    4    3   10    8   15    2]\n",
      " [   5    2   18  943    3   13    2    6   14    4]\n",
      " [   2    0    9    1  940    0    5    3    2   20]\n",
      " [  17    6    5   39    6  782   10    2   20    5]\n",
      " [  10    5   14    0    6   16  904    1    2    0]\n",
      " [   1    6   20   14   14    1    0  947    4   21]\n",
      " [  11    6    9   30    9   24   10    9  856   10]\n",
      " [   8    7    4   13   35    2    1   24    8  907]]\n",
      "\n",
      " Confusion Matrix of USPS Data: \n",
      "\n",
      "[[ 357    2  481  199  229  318   73  166   15  160]\n",
      " [  50  266  568  266  240  179   16  345   50   20]\n",
      " [ 146   91 1234  132   42  219   53   44   27   11]\n",
      " [  63   52  376  845   15  515    6   49   61   18]\n",
      " [  30   28  222  106  821  213   17  419   77   67]\n",
      " [  52   22  690  277   53  776   32   38   45   15]\n",
      " [ 162   18  939   87   72  223  435   31    3   30]\n",
      " [  25   66  185  747   62  279   10  498   94   34]\n",
      " [ 135   21  311  538  119  572   68   70  141   25]\n",
      " [  14   37  207  657  142  101    6  529  145  162]]\n"
     ]
    }
   ],
   "source": [
    "print('########################### CONFUSION MATRICES FOR SVM ###########################')\n",
    "print(\"\\nConfusion Matrix of Validation Data: \\n\\n\" + str(confusion_matrix(val_tar, val_pred_svm)))\n",
    "print(\"\\nConfusion Matrix of Testing Data: \\n\\n\" + str(confusion_matrix(test_tar, test_pred_svm)))\n",
    "print(\"\\n Confusion Matrix of USPS Data: \\n\\n\" + str(confusion_matrix(USPSTar, usps_pred_svm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimators = 100\n",
    "clf_rf = RandomForestClassifier(n_estimators = estimators, criterion = 'entropy') # using ENTROPY to measure the split\n",
    "clf_rf.fit(train_feat, train_tar) #training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Random Forest --------------------\n",
      "Number of Trees in the forest = 100\n",
      "Random Forrest Validation Accuracy:  97.11999999999999\n",
      "Random Forrest Test Accuracy:  96.81\n",
      "Random Forrest USPS Accuracy:  38.46192309615481\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Getting Validation dataset accuracy\n",
    "val_pred_rf = clf_rf.predict(val_feat)\n",
    "acc_val_rf = accuracy_score(val_tar, val_pred_rf)\n",
    "\n",
    "# Getting Testing dataset accuracy\n",
    "test_pred_rf = clf_rf.predict(test_feat)\n",
    "acc_test_rf = accuracy_score(test_tar, test_pred_rf)\n",
    "\n",
    "# Getting USPS dataset Accuracy\n",
    "usps_pred_rf = clf_rf.predict(USPSMat)\n",
    "usps_acc = accuracy_score(USPSTar, usps_pred_rf)\n",
    "\n",
    "print ('------------- Random Forest --------------------')\n",
    "print(\"Number of Trees in the forest = \" + str(estimators))\n",
    "print(\"Random Forrest Validation Accuracy: \", acc_val_rf*100)\n",
    "print(\"Random Forrest Test Accuracy: \", acc_test_rf*100)\n",
    "print(\"Random Forrest USPS Accuracy: \", usps_acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix of Validation Data: \n",
      "\n",
      "[[ 978    0    4    0    0    0    2    0    5    2]\n",
      " [   0 1052    5    1    1    1    1    0    2    1]\n",
      " [   2    0  965    1    2    1    4    8    4    3]\n",
      " [   3    0    4  999    0    8    1    4    7    4]\n",
      " [   0    5    1    1  948    1    2    1    4   20]\n",
      " [   5    2    3   15    2  869   10    1    5    3]\n",
      " [   1    0    0    0    2    2  959    0    3    0]\n",
      " [   0    5   10    1    4    0    0 1057    0   13]\n",
      " [   1    3    6    6    2    7    5    2  970    7]\n",
      " [   5    3    2   11    6    4    0    9    6  915]]\n",
      "\n",
      "Confusion Matrix of Testing Data: \n",
      "\n",
      "[[ 970    0    0    0    0    1    4    1    4    0]\n",
      " [   0 1120    3    4    0    2    3    1    2    0]\n",
      " [   5    0  999    5    2    0    4    9    7    1]\n",
      " [   1    0   13  970    0    8    0    9    7    2]\n",
      " [   1    0    2    0  952    0    4    1    2   20]\n",
      " [   2    0    0   16    3  856    6    1    5    3]\n",
      " [   7    3    1    0    3    4  938    0    2    0]\n",
      " [   2    4   19    2    1    0    0  991    1    8]\n",
      " [   5    0    3    6    4    7    4    4  931   10]\n",
      " [   7    5    1   11   13    6    1    5    6  954]]\n",
      "\n",
      " Confusion Matrix of USPS Data: \n",
      "\n",
      "[[ 608   17  221   57  450  149   60  181    3  254]\n",
      " [  31  569   71  100   58   79   19 1059   12    2]\n",
      " [  91   48 1148   91   61  146   26  382    4    2]\n",
      " [  30   10   67 1213   49  326    3  284    5   13]\n",
      " [   8  239   41   28 1025  137   16  470   12   24]\n",
      " [ 134   39  120   76   36 1386   22  175    6    6]\n",
      " [ 291   80  187   32  103  351  728  213    6    9]\n",
      " [  24  375  279  230   33  241   27  779    1   11]\n",
      " [  42   63  152  201  136 1040   60  138  138   30]\n",
      " [  16  299  200  262  237  111    6  713   58   98]]\n"
     ]
    }
   ],
   "source": [
    "print('########################### CONFUSION MATRICES FOR RANDOM FOREST CLASSIFIER ###########################')\n",
    "print(\"\\nConfusion Matrix of Validation Data: \\n\\n\" + str(confusion_matrix(val_tar, val_pred_rf)))\n",
    "print(\"\\nConfusion Matrix of Testing Data: \\n\\n\" + str(confusion_matrix(test_tar, test_pred_rf)))\n",
    "print(\"\\n Confusion Matrix of USPS Data: \\n\\n\" + str(confusion_matrix(USPSTar, usps_pred_rf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import keras.backend as K\n",
    "\n",
    "# Converting Target Variables\n",
    "train_tar_cat = keras.utils.to_categorical(train_tar, 10)\n",
    "val_tar_cat = keras.utils.to_categorical(val_tar, 10)\n",
    "test_tar_cat = keras.utils.to_categorical(test_tar, 10)\n",
    "usps_tar_cat = keras.utils.to_categorical(USPSTar, 10)\n",
    "\n",
    "# Converting a list of USPS arrays into single array\n",
    "usps_feat = np.vstack(USPSMat)\n",
    "\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Dense(units = 32, input_dim = 784, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units = 32, input_dim=32, activation='relu')) #USE 32 FOR IDEAL\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "model.compile('adam', 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "h = model.fit(train_feat, train_tar_cat,\n",
    "             batch_size = 128,\n",
    "             epochs = 20,\n",
    "             verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation CrossEntropy:  0.12830573420338331\n",
      "\n",
      "Validation Accuracy:  96.5\n",
      "\n",
      "Test CrossEntropy:  0.13939059649528934\n",
      "\n",
      "Test Accuracy:  95.88\n",
      "\n",
      "USPS CrossEntropy:  4.807576025382919\n",
      "\n",
      "USPS Accuracy:  36.4018\n"
     ]
    }
   ],
   "source": [
    "val_pred_nn = model.predict_classes(val_feat)\n",
    "test_pred_nn = model.predict_classes(test_feat)\n",
    "usps_pred_nn = model.predict_classes(usps_feat)\n",
    "\n",
    "loss_val, accuracy_val = model.evaluate(val_feat, val_tar_cat, verbose = False)\n",
    "print(\"\\nValidation CrossEntropy: \", loss_val)\n",
    "print(\"\\nValidation Accuracy: \", accuracy_val*100)\n",
    "\n",
    "loss_test, accuracy_test = model.evaluate(test_feat, test_tar_cat, verbose = False)\n",
    "print(\"\\nTest CrossEntropy: \", loss_test)\n",
    "print(\"\\nTest Accuracy: \", (round(accuracy_test*100, 4)))\n",
    "\n",
    "loss_usps, accuracy_usps = model.evaluate(usps_feat, usps_tar_cat, verbose = False)\n",
    "print(\"\\nUSPS CrossEntropy: \", loss_usps)\n",
    "print(\"\\nUSPS Accuracy: \", round(accuracy_usps*100, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix of Validation Data: \n",
      "\n",
      "[[ 971    0    2    2    0    1    5    2    4    4]\n",
      " [   0 1048    1    6    1    2    1    0    4    1]\n",
      " [   4    4  939   11    3    1    2   14   12    0]\n",
      " [   1    4    6  983    1   19    0    4    9    3]\n",
      " [   1    5    4    0  932    0    5    1    4   31]\n",
      " [   7    1    3   17    2  863   12    1    5    4]\n",
      " [   3    1    3    0    3    6  950    0    1    0]\n",
      " [   6    8    4    5    2    0    0 1060    0    5]\n",
      " [   3    9    2   11    1   15    1    3  951   13]\n",
      " [   7    2    0    9   15    2    0   10    4  912]]\n",
      "\n",
      "Confusion Matrix of Testing Data: \n",
      "\n",
      "[[ 964    0    1    1    0    2    7    3    1    1]\n",
      " [   0 1120    3    3    0    0    3    1    5    0]\n",
      " [   8    3  964   14    6    1    5   14   17    0]\n",
      " [   0    2    9  976    0   11    0    7    5    0]\n",
      " [   2    1    3    0  921    0    7    4    4   40]\n",
      " [   6    1    0   25    1  829   10    2   13    5]\n",
      " [   8    5    2    1    6   13  919    0    4    0]\n",
      " [   2   11   14    6    2    0    0  983    0   10]\n",
      " [   6    6    1   10    4   10    6    8  918    5]\n",
      " [   9    6    0    9   17    8    1    7    3  949]]\n",
      "\n",
      " Confusion Matrix of USPS Data: \n",
      "\n",
      "[[ 451   12  139  110   95  336  114  416   48  279]\n",
      " [  63  290  502  130  214  170   32  437   98   64]\n",
      " [  71   16 1377  140   17  193   97   47   32    9]\n",
      " [  23    6  180 1291    3  402   18   28   42    7]\n",
      " [  34   99   78   45  714  177   64  556  134   99]\n",
      " [  38   11  275  281   12 1195   80   58   40   10]\n",
      " [ 162    6  464   77   44  332  774   67   13   61]\n",
      " [  56   62  181  644   30   88   19  816   85   19]\n",
      " [ 132   24   87  511   83  530  103  173  314   43]\n",
      " [  30   97   86  438  103   72    8  813  163  190]]\n"
     ]
    }
   ],
   "source": [
    "print('########################### CONFUSION MATRICES FOR NEURAL NETWORKS ###########################')\n",
    "print(\"\\nConfusion Matrix of Validation Data: \\n\\n\" + str(confusion_matrix(val_tar, val_pred_nn)))\n",
    "print(\"\\nConfusion Matrix of Testing Data: \\n\\n\" + str(confusion_matrix(test_tar, test_pred_nn)))\n",
    "print(\"\\n Confusion Matrix of USPS Data: \\n\\n\" + str(confusion_matrix(USPSTar, usps_pred_nn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ENSEMBLE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Ensemble Classifier --------------------\n",
      "Ensemble Classifier Validation Accuracy:  95.87\n",
      "Ensemble Classifier Test Accuracy:  95.71\n",
      "Ensemble Classifier USPS Accuracy:  37.21186059302965\n"
     ]
    }
   ],
   "source": [
    "from statistics import mode\n",
    "\n",
    "val_final_pred = np.array([])\n",
    "test_final_pred = np.array([])\n",
    "usps_final_pred = np.array([])\n",
    "\n",
    "for i in range(0, len(val_feat)):\n",
    "    try:\n",
    "        val_final_pred = np.append(val_final_pred, mode([preds_val[i], val_pred_svm[i], val_pred_rf[i], val_pred_nn[i]]))\n",
    "    except:\n",
    "        val_final_pred = np.append(val_final_pred, max([preds_t[i], val_pred_svm[i], val_pred_rf[i], val_pred_nn[i]]))\n",
    "\n",
    "for i in range(0, len(test_feat)):\n",
    "    try:\n",
    "        test_final_pred = np.append(test_final_pred, mode([preds_t[i], test_pred_svm[i], test_pred_rf[i], test_pred_nn[i]]))\n",
    "    except:\n",
    "        test_final_pred = np.append(test_final_pred, max([preds_t[i], test_pred_svm[i], test_pred_rf[i], test_pred_nn[i]]))\n",
    "        \n",
    "for i in range(0, len(USPSTar)):\n",
    "    try:\n",
    "        usps_final_pred = np.append(usps_final_pred, mode([preds_usps[i], usps_pred_svm[i], usps_pred_rf[i], usps_pred_nn[i]]))\n",
    "    except:\n",
    "        usps_final_pred = np.append(usps_final_pred, max([preds_usps[i], usps_pred_svm[i], usps_pred_rf[i], usps_pred_nn[i]]))\n",
    "        \n",
    "print ('------------- Ensemble Classifier --------------------')\n",
    "print(\"Ensemble Classifier Validation Accuracy: \", accuracy_score(val_tar, val_final_pred)*100)\n",
    "print(\"Ensemble Classifier Test Accuracy: \", accuracy_score(test_tar, test_final_pred)*100)\n",
    "print(\"Ensemble Classifier USPS Accuracy: \", accuracy_score(USPSTar, usps_final_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################### CONFUSION MATRICES FOR ENSEMBLE CLASSIFIER ###########################\n",
      "\n",
      "Confusion Matrix of Validation Data: \n",
      "\n",
      "[[ 975    0    3    0    0    1    1    0    6    5]\n",
      " [   0 1046    2    4    1    1    2    0    7    1]\n",
      " [   6    1  933    7    4    1    4   13   16    5]\n",
      " [   2    1    4  978    1   13    0    5   17    9]\n",
      " [   2    6    1    0  924    0    4    1    6   39]\n",
      " [   8    2    3   16    1  846   17    3   11    8]\n",
      " [   3    1    0    0    2    2  954    0    2    3]\n",
      " [   3    6    6    1    1    0    0 1055    4   14]\n",
      " [   5    4    5    8    0   12    2    3  957   13]\n",
      " [   6    2    1   12    8    2    0    8    3  919]]\n",
      "\n",
      "Confusion Matrix of Testing Data: \n",
      "\n",
      "[[ 967    0    1    0    0    1    6    1    4    0]\n",
      " [   0 1114    3    5    0    0    3    1    9    0]\n",
      " [   7    0  969   14    3    0   11    8   19    1]\n",
      " [   1    1    7  975    0    3    2    8   10    3]\n",
      " [   2    0    4    0  927    0    6    1    4   38]\n",
      " [   5    1    0   21    1  822   15    3   18    6]\n",
      " [   8    4    1    0    2    7  933    0    3    0]\n",
      " [   3    6   17    3    0    0    0  979    3   17]\n",
      " [   8    2    3    5    4    4    5    6  927   10]\n",
      " [  11    6    0   11    9    1    1    6    6  958]]\n",
      "\n",
      " Confusion Matrix of USPS Data: \n",
      "\n",
      "[[ 625    2  255   73  192  197   75  253   20  308]\n",
      " [  73  303  395  158   68  126   20  696  141   20]\n",
      " [ 127   13 1321   99   18  162   62  145   43    9]\n",
      " [  61    2  127 1229    5  354   14  124   64   20]\n",
      " [  39   82   76   38  829  133   31  498  194   80]\n",
      " [ 109    8  287  193   15 1147   62  103   63   13]\n",
      " [ 288   11  466   48   41  211  735  138   24   38]\n",
      " [  41  107  185  560   22  162   35  701  159   28]\n",
      " [ 125   18  136  364   61  649  106  131  366   44]\n",
      " [  35  106  134  444  102   55    5  699  234  186]]\n"
     ]
    }
   ],
   "source": [
    "print('########################### CONFUSION MATRICES FOR ENSEMBLE CLASSIFIER ###########################')\n",
    "print(\"\\nConfusion Matrix of Validation Data: \\n\\n\" + str(confusion_matrix(val_tar, val_final_pred)))\n",
    "print(\"\\nConfusion Matrix of Testing Data: \\n\\n\" + str(confusion_matrix(test_tar, test_final_pred)))\n",
    "print(\"\\n Confusion Matrix of USPS Data: \\n\\n\" + str(confusion_matrix(USPSTar, usps_final_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
